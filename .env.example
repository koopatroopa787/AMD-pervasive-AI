# ===================================
# AI Video Processor Configuration
# ===================================
# Copy this file to .env and configure your settings

# ===================================
# Paths Configuration
# ===================================
# Path to Vosk speech recognition model
VOSK_MODEL_PATH=./models/vosk-model-en-us-0.22-lgraph

# Directory for output files
OUTPUT_DIRECTORY=./output_directory

# Temporary files directory
TEMP_DIRECTORY=./streamlit

# ===================================
# LLM Provider Configuration
# ===================================
# Choose your LLM provider: huggingface, openai, anthropic, ollama, groq
LLM_PROVIDER=huggingface

# ===================================
# Hugging Face Configuration (Local/Cloud Models)
# ===================================
HF_MODEL_ID=mistralai/Mistral-7B-Instruct-v0.2
# Other good options:
# - meta-llama/Llama-3.1-8B-Instruct
# - mistralai/Mixtral-8x7B-Instruct-v0.1
# - microsoft/phi-3-mini-4k-instruct

# Enable 4-bit quantization for memory efficiency
HF_USE_QUANTIZATION=true
HF_LOAD_IN_4BIT=true

# ===================================
# OpenAI Configuration
# ===================================
# Get your API key from: https://platform.openai.com/api-keys
OPENAI_API_KEY=your_openai_api_key_here
OPENAI_MODEL=gpt-4
# Options: gpt-4, gpt-4-turbo, gpt-3.5-turbo
# For OpenAI-compatible APIs (e.g., Azure OpenAI), set base URL:
# OPENAI_BASE_URL=https://your-endpoint.openai.azure.com/

# ===================================
# Anthropic Configuration
# ===================================
# Get your API key from: https://console.anthropic.com/
ANTHROPIC_API_KEY=your_anthropic_api_key_here
ANTHROPIC_MODEL=claude-3-5-sonnet-20241022
# Options: claude-3-5-sonnet-20241022, claude-3-opus-20240229, claude-3-sonnet-20240229

# ===================================
# Ollama Configuration (Local Models)
# ===================================
# Make sure Ollama is running: ollama serve
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=llama3.1
# Options: llama3.1, llama2, mistral, mixtral, phi3, etc.
# Install models with: ollama pull llama3.1

# ===================================
# Groq Configuration (Fast API)
# ===================================
# Get your API key from: https://console.groq.com/
GROQ_API_KEY=your_groq_api_key_here
GROQ_MODEL=mixtral-8x7b-32768
# Options: mixtral-8x7b-32768, llama3-70b-8192, llama3-8b-8192

# ===================================
# LLM General Settings
# ===================================
# Maximum tokens to generate per request
LLM_MAX_TOKENS=200

# Temperature for text generation (0.0 = deterministic, 2.0 = very creative)
LLM_TEMPERATURE=0.7

# Maximum words per text chunk for summarization
LLM_CHUNK_SIZE=1000

# ===================================
# Video Upscaling Configuration
# ===================================
UPSCALE_MODEL_ID=ai-forever/Real-ESRGAN
UPSCALE_MODEL_FILENAME=RealESRGAN_x4.pth

# Upscaling factor: 2, 4, or 8
UPSCALE_SCALE=4

# Maximum video duration to process (seconds, blank = no limit)
UPSCALE_MAX_DURATION=15

# Number of parallel workers for frame processing
UPSCALE_MAX_WORKERS=4

# ===================================
# Translation Configuration
# ===================================
TRANSLATION_MODEL_ID=facebook/hf-seamless-m4t-medium

# Supported language codes (comma-separated)
SUPPORTED_LANGUAGES=eng,fra,deu,spa,ita,por,rus,zho,jpn,kor

# ===================================
# Application Settings
# ===================================
# Enable model caching for better performance
ENABLE_CACHING=true

# Enable detailed logging
ENABLE_LOGGING=true

# Log level: DEBUG, INFO, WARNING, ERROR, CRITICAL
LOG_LEVEL=INFO

# Use GPU acceleration if available
USE_GPU=true

# UI Customization
PAGE_TITLE=AI Video Processor
PAGE_ICON=ðŸŽ¬
